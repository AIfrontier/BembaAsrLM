{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N-gram-LM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1EYA9-qRZebCUjd_-gg6MK9-pbceakSN4",
      "authorship_tag": "ABX9TyNKTGytktklTVo7AQorTf/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csikasote/BembaASR/blob/main/notebooks/N_gram_LM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtiGHCSOy3Oo"
      },
      "source": [
        "#Language Model Using KenLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82XhzF1o3_aJ"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!rm -rf /content/sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu035PZPyt4Z"
      },
      "source": [
        "# STEP 1: DOWNLOAD AND INSTALL KENLM DEPENDENCIES\n",
        "!sudo apt update && apt install -y --no-install-recommends \\\n",
        "build-essential cmake \\\n",
        "libboost-system-dev \\\n",
        "libboost-thread-dev \\\n",
        "libboost-program-options-dev \\\n",
        "libboost-test-dev \\\n",
        "libeigen3-dev \\\n",
        "zlib1g-dev \\\n",
        "libbz2-dev \\\n",
        "liblzma-dev\n",
        "clear_output() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hFjPNiUy6iz"
      },
      "source": [
        "# STEP 2: DOWNLOAD AND INSTALL KENLM\n",
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz && \\\n",
        "mkdir kenlm/build && \\\n",
        "cd kenlm/build && \\\n",
        "cmake .. && \\\n",
        "make -j4\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt139Fo615UF"
      },
      "source": [
        "# STEP 3: DOWNLOAD AND INSTALL GITHUB LFS \n",
        "%cd /content/\n",
        "!wget https://github.com/git-lfs/git-lfs/releases/download/v2.11.0/git-lfs-linux-amd64-v2.11.0.tar.gz\n",
        "!tar xvf /content/git-lfs-linux-amd64-v2.11.0.tar.gz -C /content\n",
        "!sudo ./install.sh\n",
        "!rm -rf git-lfs-linux-amd64-v2.11.0.tar.gz\n",
        "!rm -rf /content/README.md\n",
        "!rm -rf /content/CHANGELOG.md\n",
        "!rm -rf /content/install.sh\n",
        "!rm -rf /content/git-lfs\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JejrdJBC18Ka",
        "outputId": "57c49b93-43d5-4397-dccb-94dd356e5ebd"
      },
      "source": [
        "# STEP 4: DOWNLOAD AND INSTALL DEEPSPEECH V0.8.2\n",
        "%cd /content/\n",
        "!git lfs install\n",
        "!git clone https://github.com/mozilla/DeepSpeech\n",
        "%cd DeepSpeech/\n",
        "!git checkout -b v0.8.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Git LFS initialized.\n",
            "Cloning into 'DeepSpeech'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 23116 (delta 11), reused 11 (delta 5), pack-reused 23085\u001b[K\n",
            "Receiving objects: 100% (23116/23116), 49.01 MiB | 23.88 MiB/s, done.\n",
            "Resolving deltas: 100% (15907/15907), done.\n",
            "/content/DeepSpeech\n",
            "Switched to a new branch 'v0.8.2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "toa0FJxyLQHh",
        "outputId": "88d1f7d8-e330-42cd-b6cc-4bbb524d2f4c"
      },
      "source": [
        "# STEP 4.1: DOWNLOAD AND INSTALL DEPENDENCIES\n",
        "%cd '/content/DeepSpeech'\n",
        "!pip3 install --upgrade pip==20.0.2 wheel==0.34.2 setuptools==46.1.3\n",
        "!pip3 install --upgrade -e ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepSpeech\n",
            "Collecting pip==20.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 5.5MB/s \n",
            "\u001b[?25hCollecting wheel==0.34.2\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\n",
            "Collecting setuptools==46.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/df/635cdb901ee4a8a42ec68e480c49f85f4c59e8816effbf57d9e6ee8b3588/setuptools-46.1.3-py3-none-any.whl (582kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 19.0MB/s \n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement wheel~=0.35, but you'll have wheel 0.34.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip, wheel, setuptools\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "  Found existing installation: wheel 0.36.2\n",
            "    Uninstalling wheel-0.36.2:\n",
            "      Successfully uninstalled wheel-0.36.2\n",
            "  Found existing installation: setuptools 51.1.1\n",
            "    Uninstalling setuptools-51.1.1:\n",
            "      Successfully uninstalled setuptools-51.1.1\n",
            "Successfully installed pip-20.0.2 setuptools-46.1.3 wheel-0.34.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/DeepSpeech\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.10.0a3) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.10.0a3) (3.38.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.10.0a3) (1.15.0)\n",
            "Collecting pyxdg\n",
            "  Downloading pyxdg-0.27-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.10.0a3) (0.10.0)\n",
            "Collecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting opuslib==2.0.0\n",
            "  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-2.4.0-py3-none-any.whl (282 kB)\n",
            "\u001b[K     |████████████████████████████████| 282 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting sox\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied, skipping upgrade: bs4 in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.10.0a3) (0.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.10.0a3) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.10.0a3) (2.23.0)\n",
            "Collecting numba==0.47.0\n",
            "  Downloading numba-0.47.0-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: llvmlite==0.31.0 in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.10.0a3) (0.31.0)\n",
            "Requirement already satisfied, skipping upgrade: librosa in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.10.0a3) (0.6.3)\n",
            "Collecting soundfile\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Collecting ds_ctcdecoder==0.10.0-alpha.3\n",
            "  Downloading ds_ctcdecoder-0.10.0a3-cp36-cp36m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 67.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15.4\n",
            "  Downloading tensorflow-1.15.4-cp36-cp36m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 47 kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->deepspeech-training==0.10.0a3) (2.4.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-4.7.2-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.10.0a3) (1.0.0)\n",
            "Collecting cmaes>=0.6.0\n",
            "  Downloading cmaes-0.7.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.10.0a3) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.10.0a3) (1.4.1)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.5.0-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.10.0a3) (20.8)\n",
            "Requirement already satisfied, skipping upgrade: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.10.0a3) (1.3.22)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.4.3-py2.py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->deepspeech-training==0.10.0a3) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->deepspeech-training==0.10.0a3) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->deepspeech-training==0.10.0a3) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.10.0a3) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.10.0a3) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.10.0a3) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.10.0a3) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from numba==0.47.0->deepspeech-training==0.10.0a3) (46.1.3)\n",
            "Requirement already satisfied, skipping upgrade: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.10.0a3) (2.1.9)\n",
            "Requirement already satisfied, skipping upgrade: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.10.0a3) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.10.0a3) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.10.0a3) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile->deepspeech-training==0.10.0a3) (1.14.4)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.10.0a3) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.10.0a3) (3.12.4)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.10.0a3) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.10.0a3) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.10.0a3) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.10.0a3) (0.34.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.10.0a3) (1.12.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 42.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.10.0a3) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.10.0a3) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->deepspeech-training==0.10.0a3) (3.13)\n",
            "Collecting cmd2!=0.8.3,>=0.8.0\n",
            "  Downloading cmd2-1.4.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 55.1 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.5.1-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 54.7 MB/s \n",
            "\u001b[?25hCollecting PrettyTable<0.8,>=0.7.2\n",
            "  Downloading prettytable-0.7.2.tar.bz2 (21 kB)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->deepspeech-training==0.10.0a3) (2.4.7)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.3.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "  Downloading Mako-1.1.4.tar.gz (479 kB)\n",
            "\u001b[K     |████████████████████████████████| 479 kB 43.8 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile->deepspeech-training==0.10.0a3) (2.20)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4->deepspeech-training==0.10.0a3) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->deepspeech-training==0.10.0a3) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->deepspeech-training==0.10.0a3) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna->deepspeech-training==0.10.0a3) (0.2.5)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.1.tar.gz (20 kB)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna->deepspeech-training==0.10.0a3) (20.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna->deepspeech-training==0.10.0a3) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna->deepspeech-training==0.10.0a3) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2!=0.8.3,>=0.8.0->cliff->optuna->deepspeech-training==0.10.0a3) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2!=0.8.3,>=0.8.0->cliff->optuna->deepspeech-training==0.10.0a3) (3.7.4.3)\n",
            "Building wheels for collected packages: opuslib, gast, PrettyTable, Mako, pyperclip\n",
            "  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11009 sha256=22d780183a7b38800b966bd0d8c234bbafd6778eb0bda4fc4b44cb306f9e5e3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/01/88/37797e9e9d157a33eefed22a46aa0bf5044effcec6a9181e41\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=c17c7da0c13baefc0c61560cb1842cfab875ab1b50e23557e617d1336ccee836\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PrettyTable: filename=prettytable-0.7.2-py3-none-any.whl size=13698 sha256=63bc8853ac4742983f22cfe3ff01fe328c0763da69aa8630eb7a5ace70d6cb14\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/15/c3/5f28b42ae9c81638570b8b7ed654e0f98c5fdc08875869511b\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Mako: filename=Mako-1.1.4-py2.py3-none-any.whl size=75675 sha256=4ab539c6d235ea1f2194cca45cf56926b10039a543a47cd0931b0f952e5a1b56\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/ee/c2/9651c6b977f9d2a1bb766970d190f71213e2ca47b36d8dc488\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.1-py3-none-any.whl size=11118 sha256=584f350386bb6ec728376542d4c6af60de4f6c8da8dae10817f45dbadc8feb21\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/27/28/fa23ac551b4fad562edc8d50a4ae1182f31408aae1d6027c39\n",
            "Successfully built opuslib gast PrettyTable Mako pyperclip\n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numba!=0.47,>=0.46, but you'll have numba 0.47.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.4 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pyxdg, attrdict, semver, opuslib, colorlog, cmaes, colorama, pyperclip, cmd2, pbr, PrettyTable, stevedore, cliff, Mako, python-editor, alembic, optuna, sox, numba, soundfile, ds-ctcdecoder, keras-applications, gast, tensorflow-estimator, tensorboard, tensorflow, deepspeech-training\n",
            "  Attempting uninstall: PrettyTable\n",
            "    Found existing installation: prettytable 2.0.0\n",
            "    Uninstalling prettytable-2.0.0:\n",
            "      Successfully uninstalled prettytable-2.0.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.48.0\n",
            "    Uninstalling numba-0.48.0:\n",
            "      Successfully uninstalled numba-0.48.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.4.0\n",
            "    Uninstalling tensorboard-2.4.0:\n",
            "      Successfully uninstalled tensorboard-2.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.4.0\n",
            "    Uninstalling tensorflow-2.4.0:\n",
            "      Successfully uninstalled tensorflow-2.4.0\n",
            "  Running setup.py develop for deepspeech-training\n",
            "Successfully installed Mako-1.1.4 PrettyTable-0.7.2 alembic-1.4.3 attrdict-2.0.1 cliff-3.5.0 cmaes-0.7.0 cmd2-1.4.0 colorama-0.4.4 colorlog-4.7.2 deepspeech-training ds-ctcdecoder-0.10.0a3 gast-0.2.2 keras-applications-1.0.8 numba-0.47.0 optuna-2.4.0 opuslib-2.0.0 pbr-5.5.1 pyperclip-1.8.1 python-editor-1.0.4 pyxdg-0.27 semver-2.13.0 soundfile-0.10.3.post1 sox-1.4.1 stevedore-3.3.0 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LDWqvStLTD3"
      },
      "source": [
        "# STEP 4.2: INSTALL REQUIRED TENSORFLOW VERSION = 'tensorflow-gpu==1.15.2'\n",
        "!pip3 uninstall tensorflow\n",
        "!pip3 install 'tensorflow-gpu==1.15.2'\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9U7xtTDLTOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1fc3a1f-533d-4648-f803-b666d65a359d"
      },
      "source": [
        "# STEP 4.3 INSTALL DEEPSPEECH-GPU\n",
        "!pip3 install deepspeech\n",
        "!pip3 install deepspeech-gpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deepspeech in /usr/local/lib/python3.6/dist-packages (0.9.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepspeech) (1.19.5)\n",
            "Requirement already satisfied: deepspeech-gpu in /usr/local/lib/python3.6/dist-packages (0.9.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepspeech-gpu) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soEy1-if2hja",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cbb14a6-d59b-4478-c337-d227314cc5ed"
      },
      "source": [
        "# STEP 5B: DOWNLOAD AND EXTRACT THE NATIVE CLIENT 'native_client.amd64.cuda.linux.tar.xz' TO BUIDING THE SCORER\n",
        "%cd '/content/DeepSpeech/data/lm/'\n",
        "!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/native_client.amd64.cuda.linux.tar.xz\n",
        "!tar xvf native_client.amd64.cuda.linux.tar.xz\n",
        "!rm -rf native_client.amd64.cuda.linux.tar.xz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DeepSpeech/data/lm\n",
            "--2021-01-17 12:53:50--  https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/native_client.amd64.cuda.linux.tar.xz\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/adcac680-e49c-11ea-949c-0c380912af4a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210117%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210117T125353Z&X-Amz-Expires=300&X-Amz-Signature=bfa6536e4159717b22791defa932bd3ebe0e7cfc604d45eb21bb8991808838e4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Dnative_client.amd64.cuda.linux.tar.xz&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-01-17 12:53:53--  https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/adcac680-e49c-11ea-949c-0c380912af4a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210117%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210117T125353Z&X-Amz-Expires=300&X-Amz-Signature=bfa6536e4159717b22791defa932bd3ebe0e7cfc604d45eb21bb8991808838e4&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Dnative_client.amd64.cuda.linux.tar.xz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.49.108\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.49.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11347900 (11M) [application/octet-stream]\n",
            "Saving to: ‘native_client.amd64.cuda.linux.tar.xz’\n",
            "\n",
            "native_client.amd64 100%[===================>]  10.82M  36.9MB/s    in 0.3s    \n",
            "\n",
            "2021-01-17 12:53:53 (36.9 MB/s) - ‘native_client.amd64.cuda.linux.tar.xz’ saved [11347900/11347900]\n",
            "\n",
            "libdeepspeech.so\n",
            "generate_scorer_package\n",
            "LICENSE\n",
            "deepspeech\n",
            "deepspeech.h\n",
            "README.mozilla\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKrWBhfDlY-0"
      },
      "source": [
        "STEP 1: [TRANSCRIPTS ONLY]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-yjqyAR0q88",
        "outputId": "82336139-fbd2-4825-9a9a-031a8d32526f"
      },
      "source": [
        "!python3 /content/DeepSpeech/data/lm/generate_lm.py \\\n",
        "  --input_txt /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts/transcripts.txt \\\n",
        "  --output_dir /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts \\\n",
        "  --top_k 50000 \\\n",
        "  --kenlm_bins /content/kenlm/build/bin \\\n",
        "  --arpa_order 5 \\\n",
        "  --max_arpa_memory \"85%\" \\\n",
        "  --arpa_prune \"0|0|1\" \\\n",
        "  --binary_a_bits 255 \\\n",
        "  --binary_q_bits 8 \\\n",
        "  --binary_type trie"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Converting to lowercase and counting word occurrences ...\n",
            "| |  #                                             | 13460 Elapsed Time: 0:00:00\n",
            "\n",
            "Saving top 50000 words ...\n",
            "\n",
            "Calculating word statistics ...\n",
            "  Your text file has 123295 words in total\n",
            "  It has 27209 unique words\n",
            "  Your top-50000 words are 100.0000 percent of all words\n",
            "  Your most common word \"mu\" occurred 2341 times\n",
            "  The least common word in your top-k is \"kwanga\" with 1 times\n",
            "  The first word with 2 occurrences is \"lumbanyeni\" at place 8784\n",
            "\n",
            "Creating ARPA file ...\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts/lower.txt.gz\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 2043600896 bytes == 0x55cd7ecb2000 @  0x7fae672f01e7 0x55cd7c3077a2 0x55cd7c2a251e 0x55cd7c2812eb 0x55cd7c26d066 0x7fae65489bf7 0x55cd7c26ebaa\n",
            "tcmalloc: large alloc 9536798720 bytes == 0x55cdf89a0000 @  0x7fae672f01e7 0x55cd7c3077a2 0x55cd7c2f67ca 0x55cd7c2f7208 0x55cd7c281308 0x55cd7c26d066 0x7fae65489bf7 0x55cd7c26ebaa\n",
            "****************************************************************************************************\n",
            "Unigram tokens 123295 types 27212\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:326544 2:1132218112 3:2122909056 4:3396654336 5:4953454592\n",
            "tcmalloc: large alloc 4953456640 bytes == 0x55cd7eba8000 @  0x7fae672f01e7 0x55cd7c3077a2 0x55cd7c2f67ca 0x55cd7c2f7208 0x55cd7c2818d7 0x55cd7c26d066 0x7fae65489bf7 0x55cd7c26ebaa\n",
            "tcmalloc: large alloc 2122915840 bytes == 0x55cee97c2000 @  0x7fae672f01e7 0x55cd7c3077a2 0x55cd7c2f67ca 0x55cd7c2f7208 0x55cd7c281cdd 0x55cd7c26d066 0x7fae65489bf7 0x55cd7c26ebaa\n",
            "tcmalloc: large alloc 3396657152 bytes == 0x55d031934000 @  0x7fae672f01e7 0x55cd7c3077a2 0x55cd7c2f67ca 0x55cd7c2f7208 0x55cd7c281cdd 0x55cd7c26d066 0x7fae65489bf7 0x55cd7c26ebaa\n",
            "Statistics:\n",
            "1 27212 D1=0.751136 D2=1.10527 D3+=1.44432\n",
            "2 90552 D1=0.867574 D2=1.25496 D3+=1.29564\n",
            "3 7692/110323 D1=0.946295 D2=1.4079 D3+=1.60768\n",
            "4 4163/104105 D1=0.978009 D2=1.65198 D3+=1.97202\n",
            "5 2943/92562 D1=0.947477 D2=1.66587 D3+=2.54573\n",
            "Memory estimate for binary LM:\n",
            "type      kB\n",
            "probing 3142 assuming -p 1.5\n",
            "probing 3849 assuming -r models -p 1.5\n",
            "trie    1791 without quantization\n",
            "trie    1202 assuming -q 8 -b 8 quantization \n",
            "trie    1675 assuming -a 22 array pointer compression\n",
            "trie    1086 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:326544 2:1448832 3:153840 4:99912 5:82404\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "***#################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:326544 2:1448832 3:153840 4:99912 5:82404\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:15077240 kB\tVmRSS:2029492 kB\tRSSMax:2042024 kB\tuser:0.40537\tsys:1.00925\tCPU:1.41466\treal:1.56069\n",
            "\n",
            "Filtering ARPA file using vocabulary of top-k words ...\n",
            "Reading /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts/lm.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "\n",
            "Building lm.binary ...\n",
            "Reading /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts/lm_filtered.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Identifying n-grams omitted by SRI\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Quantizing\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Writing trie\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hFuxJm22zLA",
        "outputId": "3470097b-2e78-42da-f162-c4839ccbdf1e"
      },
      "source": [
        "# STEP 5C: GENERATE 'kenlm.scorer' FROM 'lm.binary' and 'vocab-20000.txt' USING THE 'generate_scorer_package'\n",
        "!/content/DeepSpeech/data/lm/generate_scorer_package \\\n",
        "  --alphabet /content/DeepSpeech/data/alphabet.txt \\\n",
        "  --lm /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts/lm.binary \\\n",
        "  --vocab /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts/vocab-50000.txt \\\n",
        "  --package /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts/kenlm.scorer \\\n",
        "  --default_alpha 0.93128903105002 \\\n",
        "  --default_beta 1.183413758151028 \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27209 unique words read from vocabulary file.\n",
            "Doesn't look like a character based (Bytes Are All You Need) model.\n",
            "--force_utf8 was not specified, using value infered from vocabulary contents: false\n",
            "Package created in /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts/kenlm.scorer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc5UZIhfnRco"
      },
      "source": [
        "STEP 2: [TRANNSCRIPT + JW]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj8HyJuh3JQU",
        "outputId": "f9b84682-c3b9-437c-933c-2b455817f6fa"
      },
      "source": [
        "!python3 /content/DeepSpeech/data/lm/generate_lm.py \\\n",
        "  --input_txt /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts_plus_jw/transcripts_plus_jw.txt \\\n",
        "  --output_dir /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts_plus_jw \\\n",
        "  --top_k 500000 \\\n",
        "  --kenlm_bins /content/kenlm/build/bin/ \\\n",
        "  --arpa_order 5 \\\n",
        "  --max_arpa_memory \"85%\" \\\n",
        "  --arpa_prune \"0|0|1\" \\\n",
        "  --binary_a_bits 255 \\\n",
        "  --binary_q_bits 8 \\\n",
        "  --binary_type trie"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Converting to lowercase and counting word occurrences ...\n",
            "| |    #                                          | 403451 Elapsed Time: 0:00:08\n",
            "\n",
            "Saving top 500000 words ...\n",
            "\n",
            "Calculating word statistics ...\n",
            "  Your text file has 5797900 words in total\n",
            "  It has 188782 unique words\n",
            "  Your top-500000 words are 100.0000 percent of all words\n",
            "  Your most common word \"mu\" occurred 133034 times\n",
            "  The least common word in your top-k is \"abasonkesha\" with 1 times\n",
            "  The first word with 2 occurrences is \"abashaitumpile\" at place 90702\n",
            "\n",
            "Creating ARPA file ...\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts_plus_jw/lower.txt.gz\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 2043600896 bytes == 0x558651e1e000 @  0x7fb5ab9a11e7 0x558650c277a2 0x558650bc251e 0x558650ba12eb 0x558650b8d066 0x7fb5a9b3abf7 0x558650b8ebaa\n",
            "tcmalloc: large alloc 9536798720 bytes == 0x5586cbb0c000 @  0x7fb5ab9a11e7 0x558650c277a2 0x558650c167ca 0x558650c17208 0x558650ba1308 0x558650b8d066 0x7fb5a9b3abf7 0x558650b8ebaa\n",
            "****************************************************************************************************\n",
            "Unigram tokens 5797900 types 188785\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:2265420 2:1132028800 3:2122554240 4:3396086528 5:4952626688\n",
            "tcmalloc: large alloc 4952629248 bytes == 0x558651d14000 @  0x7fb5ab9a11e7 0x558650c277a2 0x558650c167ca 0x558650c17208 0x558650ba18d7 0x558650b8d066 0x7fb5a9b3abf7 0x558650b8ebaa\n",
            "tcmalloc: large alloc 2122555392 bytes == 0x5587bca10000 @  0x7fb5ab9a11e7 0x558650c277a2 0x558650c167ca 0x558650c17208 0x558650ba1cdd 0x558650b8d066 0x7fb5a9b3abf7 0x558650b8ebaa\n",
            "tcmalloc: large alloc 3396091904 bytes == 0x558904aa0000 @  0x7fb5ab9a11e7 0x558650c277a2 0x558650c167ca 0x558650c17208 0x558650ba1cdd 0x558650b8d066 0x7fb5a9b3abf7 0x558650b8ebaa\n",
            "Statistics:\n",
            "1 188785 D1=0.660394 D2=1.0618 D3+=1.41586\n",
            "2 1483331 D1=0.785491 D2=1.11023 D3+=1.37346\n",
            "3 519353/3196437 D1=0.866186 D2=1.1904 D3+=1.39058\n",
            "4 377207/4132032 D1=0.923964 D2=1.28557 D3+=1.43256\n",
            "5 233958/4363366 D1=0.934656 D2=1.2522 D3+=1.36022\n",
            "Memory estimate for binary LM:\n",
            "type    MB\n",
            "probing 63 assuming -p 1.5\n",
            "probing 77 assuming -r models -p 1.5\n",
            "trie    34 without quantization\n",
            "trie    20 assuming -q 8 -b 8 quantization \n",
            "trie    30 assuming -a 22 array pointer compression\n",
            "trie    16 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:2265420 2:23733296 3:10387060 4:9052968 5:6550824\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "*****###############################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:2265420 2:23733296 3:10387060 4:9052968 5:6550824\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:15076684 kB\tVmRSS:2393848 kB\tRSSMax:2395792 kB\tuser:9.89478\tsys:2.60502\tCPU:12.4998\treal:12.584\n",
            "\n",
            "Filtering ARPA file using vocabulary of top-k words ...\n",
            "Reading /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts_plus_jw/lm.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "\n",
            "Building lm.binary ...\n",
            "Reading /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts_plus_jw/lm_filtered.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Identifying n-grams omitted by SRI\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Quantizing\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Writing trie\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4KaCy3L3Zyz",
        "outputId": "d0278849-f0bd-42d6-c142-59058c262a6a"
      },
      "source": [
        "# STEP 6B: GENERATE 'kenlm.scorer' FROM 'lm.binary' and 'vocab-20000.txt' USING THE 'generate_scorer_package'\n",
        "!/content/DeepSpeech/data/lm/generate_scorer_package \\\n",
        "  --alphabet /content/DeepSpeech/data/alphabet.txt \\\n",
        "  --lm /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts_plus_jw/lm.binary \\\n",
        "  --vocab /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts_plus_jw/vocab-500000.txt \\\n",
        "  --package /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts_plus_jw/kenlm.scorer \\\n",
        "  --default_alpha 0.93128903105002 \\\n",
        "  --default_beta 1.183413758151028 \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "188782 unique words read from vocabulary file.\n",
            "Doesn't look like a character based (Bytes Are All You Need) model.\n",
            "--force_utf8 was not specified, using value infered from vocabulary contents: false\n",
            "Package created in /content/drive/MyDrive/deepspeech/language_model/5NG/transcripts_plus_jw/kenlm.scorer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgRUo4t_S14e"
      },
      "source": [
        "# STEP 9: DOWNLOAD DEEPSPEECHV0.8.2 CHECKPOINT\n",
        "%cd /content/\n",
        "!mkdir checkpoint_dir\n",
        "%cd '/content/checkpoint_dir'\n",
        "!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.8.2/deepspeech-0.8.2-checkpoint.tar.gz\n",
        "!tar xvfz deepspeech-0.8.2-checkpoint.tar.gz\n",
        "!rm -rf deepspeech-0.8.2-checkpoint.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfxQmICj2_0H"
      },
      "source": [
        "# STEP 9: TUNE THE LANGUAGE MODEL TO FIND GOOD 'alpha' && 'beta' DEFAULT PARAMETERS FOR THE SCORER\n",
        "!python3 /content/DeepSpeech/lm_optimizer.py \\\n",
        "  --alphabet_config_path /content/DeepSpeech/data/alphabet.txt \\\n",
        "  --checkpoint_dir /content/checkpoint_dir/deepspeech-0.8.2-checkpoint \\\n",
        "  --test_files /content/deepspeech_dts/BembaSpeech/bembaspeech/test.csv \\\n",
        "  --scorer /content/drive/MyDrive/datastore/kenlm.scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZx72v5f_vLN"
      },
      "source": [
        "# STEP 10: RE-GENERATE 'kenlm.scorer' FROM 'lm.binary' and 'vocab-20000.txt' USING THE 'generate_scorer_package'\n",
        "!/content/DeepSpeech/data/lm/generate_scorer_package \\\n",
        "  --alphabet /content/DeepSpeech/data/alphabet.txt \\\n",
        "  --lm /content/drive/MyDrive/datastore/lm.binary \\\n",
        "  --vocab /content/drive/MyDrive/datastore/vocab-20000.txt \\\n",
        "  --package /content/drive/MyDrive/datastore/kenlm.scorer \\\n",
        "  --default_alpha 0.93128903105002 \\\n",
        "  --default_beta 1.183413758151028 \\"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}